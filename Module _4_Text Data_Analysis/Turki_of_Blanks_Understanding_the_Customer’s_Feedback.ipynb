{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59bfe54-9fdb-47ae-ac55-126d071f53ec",
   "metadata": {
    "id": "Q24hSxA4tA6n"
   },
   "source": [
    "# Text classification: Understanding the Customer's Feedback\n",
    "\n",
    "---\n",
    "\n",
    "Text classification is one of the important tasks of text mining\n",
    "\n",
    "![alt text](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1535125878/NLTK3_zwbdgg.png)\n",
    "\n",
    "In this notebook, we will perform Sentiment Analysis on IMDB movies reviews. Sentiment Analysis is the art of extracting people's opinion from digital text. We will use a regression model from Scikit-Learn able to predict the sentiment given a movie review.\n",
    "\n",
    "We will use [the IMDB movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/), which consists of 50,000 movies review (50% are positive, 50% are negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7c2bce-8871-4b28-8499-6eae37f07efb",
   "metadata": {
    "id": "0O1jA8byt4bV"
   },
   "source": [
    "The libraries needed in this exercise are:\n",
    "* [Numpy](http://www.numpy.org/) — a package for scientific computing.\n",
    "* [Pandas](https://pandas.pydata.org/) — a library providing high-performance, easy-to-use data structures and data analysis tools for the Python\n",
    "* [Matplotlib](https://matplotlib.org/) — a package for plotting & visualizations.\n",
    "* [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "* [NLTK](http://www.nltk.org/) — a platform to work with natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f72df-495f-4c8f-a781-c2902206ff88",
   "metadata": {
    "id": "844CS6rf57X7"
   },
   "source": [
    "##Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd683f0-b83a-405b-a756-9aa13a5528a2",
   "metadata": {
    "id": "QAt6rj955meo"
   },
   "source": [
    "### Importing the libraries and necessary dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdd05164-ce27-4f82-8252-7b3486288569",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionCancelledAt": null,
    "executionTime": 2296,
    "id": "RRN4WqkltlB5",
    "lastExecutedAt": 1699991733361,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import numpy as np\nimport pandas as pd\nimport nltk\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\n\n# download Punkt Sentence Tokenizer\nnltk.download('punkt')\n# download stopwords\nnltk.download('stopwords')",
    "outputId": "385d7e67-9311-4679-a426-5afa19b71594",
    "outputsMetadata": {
     "0": {
      "height": 95,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 20:07:56.979485: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC-NOTICE: GPU memory for this assignment is capped at 1024MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 20:07:59.659994: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "[nltk_data] Downloading package punkt to /voc/work/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /voc/work/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "# download Punkt Sentence Tokenizer\n",
    "nltk.download('punkt')\n",
    "# download stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef6fe8-712a-49e7-923c-f3e2608e6b48",
   "metadata": {
    "id": "7duM74C95rhN"
   },
   "source": [
    "### Loading the dataset in our directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5288c1-24df-4d0c-a07e-b53a57485c86",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionCancelledAt": null,
    "executionTime": 2112,
    "id": "c48UYWDcg3hR",
    "lastExecutedAt": 1699991735473,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# download IMDB dataset\n!wget \"https://raw.githubusercontent.com/javaidnabi31/Word-Embeddding-Sentiment-Classification/master/movie_data.csv\" -O \"movie_data.csv\"\n\n# list files in current directory\n!ls -lah",
    "outputId": "e061e216-4329-4a42-a80c-7c85a068c99f",
    "outputsMetadata": {
     "0": {
      "height": 290,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-14 20:08:01--  https://raw.githubusercontent.com/javaidnabi31/Word-Embeddding-Sentiment-Classification/master/movie_data.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 65862309 (63M) [text/plain]\n",
      "Saving to: ‘movie_data.csv’\n",
      "\n",
      "movie_data.csv      100%[===================>]  62.81M   169MB/s    in 0.4s    \n",
      "\n",
      "2023-11-14 20:08:03 (169 MB/s) - ‘movie_data.csv’ saved [65862309/65862309]\n",
      "\n",
      "total 64M\n",
      "drwxrwx--- 13 labsuser       48 6.0K Nov 14 20:07  .\n",
      "drwxr-xr-x  6 root     root     4.0K Nov 14 20:04  ..\n",
      "drwxr-xr-x  5 labsuser labsuser 6.0K Nov 13 19:20  .cache\n",
      "drwxr-xr-x  3 labsuser labsuser 6.0K Nov  9 05:30  .config\n",
      "-rw-rw-r--  1 labsuser       48   52 Nov  9 02:27  .gitconfig\n",
      "drwxr-xr-x  2 labsuser labsuser 6.0K Nov 14 20:06  .ipynb_checkpoints\n",
      "drwxr-xr-x  5 labsuser labsuser 6.0K Nov  9 05:17  .ipython\n",
      "drwxr-xr-x  3 labsuser labsuser 6.0K Nov 13 19:18  .jupyter\n",
      "drwxr-xr-x  2 labsuser labsuser 6.0K Nov  9 05:32  .keras\n",
      "drwxr-xr-x  5 labsuser labsuser 6.0K Nov 13 19:21  .local\n",
      "drwxr-x---  2 labsuser       48 6.0K Nov  9 02:27  .ssh\n",
      "dr-xr-xr-x  2 root     root     6.0K Nov  9 02:27  .voc\n",
      "-rw-r--r--  1 labsuser labsuser  180 Nov 14 20:08  .wget-hsts\n",
      "-rw-r--r--  1 labsuser labsuser  46K Nov 14 04:05  205Turki_of_Blanks_Understanding_the_Customer’s_Feedback.ipynb\n",
      "-rw-r--r--  1 labsuser labsuser  66K Nov 14 01:59 'Blanks_Deep_sentiment_analysis_(1final)-2.ipynb'\n",
      "-rw-r--r--  1 labsuser labsuser 160K Nov  9 05:33  Blanks_Positive_Stroke_Cases.ipynb\n",
      "-rw-r--r--  1 labsuser labsuser  25K Nov 14 20:07  Turki_of_Blanks_Understanding_the_Customer’s_Feedback.ipynb\n",
      "drwxr-xr-x  2 labsuser labsuser 6.0K Nov  9 05:17 'Untitled Folder'\n",
      "-rw-r--r--  1 labsuser labsuser  612 Nov 14 01:11  Untitled.ipynb\n",
      "-rw-r--r--  1 labsuser labsuser  612 Nov 14 20:05  Untitled1.ipynb\n",
      "-rw-r--r--  1 labsuser labsuser 310K Nov  9 05:17  healthcare-dataset-stroke-data.csv\n",
      "-rw-r--r--  1 labsuser labsuser  63M Nov 14 20:08  movie_data.csv\n",
      "drwxr-xr-x  4 labsuser labsuser 6.0K Nov 13 19:32  nltk_data\n"
     ]
    }
   ],
   "source": [
    "# download IMDB dataset\n",
    "!wget \"https://raw.githubusercontent.com/javaidnabi31/Word-Embeddding-Sentiment-Classification/master/movie_data.csv\" -O \"movie_data.csv\"\n",
    "\n",
    "# list files in current directory\n",
    "!ls -lah"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbebb1ee-de93-497d-87ae-87a2da1b5aed",
   "metadata": {
    "id": "77spW4xt5y4R"
   },
   "source": [
    "###Reading the dataset file and getting info on it\n",
    "**Question 1:** Use pandas to read the csv file and display the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a09be8-77b5-46e9-99f9-2507518b2390",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionCancelledAt": null,
    "executionTime": 514,
    "id": "R0A5QhDlteWj",
    "lastExecutedAt": 1699991735987,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# path to IMDB dataseet\ndataset_path = 'movie_data.csv'\n\n# read file (dataset) into our program using pandas\ndata = pd.read_csv('movie_data.csv')\n\n# display first 5 rows\ndata.head(5)",
    "outputId": "d6099104-684e-44dd-dfca-092fc913c528"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I went and saw this movie last night after bei...          1\n",
       "1  Actor turned director Bill Paxton follows up h...          1\n",
       "2  As a recreational golfer with some knowledge o...          1\n",
       "3  I saw this film in a sneak preview, and it is ...          1\n",
       "4  Bill Paxton has taken the true story of the 19...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path to IMDB dataseet\n",
    "dataset_path = 'movie_data.csv'\n",
    "\n",
    "# read file (dataset) into our program using pandas\n",
    "data = pd.read_csv('movie_data.csv')\n",
    "\n",
    "# display first 5 rows\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e0f8ed-dc9c-42a2-8aea-698a02dd37a5",
   "metadata": {
    "id": "t8oHmgm-6qK2"
   },
   "source": [
    "Getting info on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad3260e0-f675-4f28-a80d-15ed1d49ac61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionCancelledAt": null,
    "executionTime": 56,
    "id": "uQVx6AhqhAiB",
    "lastExecutedAt": 1699991736043,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "data.info()",
    "outputId": "78445948-ab63-40d0-cc57-bff513bf4c30",
    "outputsMetadata": {
     "0": {
      "height": 193,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9badc03-e852-497b-b27e-fcff21cd790b",
   "metadata": {
    "id": "cPbcG_8k54JZ"
   },
   "source": [
    "A balanced dataset in sentiment analysis is a dataset which holds an equal amount of positive sentiment data and negative sentiment data, meaning 50% of the data is positive and 50% is negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ceec2f-6343-41a8-b6fa-79e691f39690",
   "metadata": {
    "id": "rgvEJ3BSK_7e"
   },
   "source": [
    "**Question 2:** Check if dataset is balanced (number of positive sentiment = number of negative sentiment) by plotting the different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43799d97-f45e-4c34-be51-ff935613aa10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionCancelledAt": null,
    "executionTime": 44,
    "id": "54jY3LxNLspq",
    "lastExecutedAt": 1699991736087,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "data['sentiment'].value_counts()",
    "outputId": "d68b9409-9bb4-4939-ca90-17b796759a26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    25000\n",
       "0    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff6802a3-b998-462a-9424-934f25a14c47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionCancelledAt": null,
    "executionTime": 220,
    "id": "yWGlbrIpLtqf",
    "lastExecutedAt": 1699991736307,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "data['sentiment'].hist()",
    "outputId": "40d27ce5-b3e9-4b2a-911e-20920c92c60e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['sentiment'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8020024-079a-4949-9dba-3cf3f4c73cbd",
   "metadata": {
    "id": "R4uAuueIwKkS"
   },
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "065ef528-eb0c-48f9-95b8-b9a84a099b44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionCancelledAt": null,
    "executionTime": 47,
    "id": "qCxs0pSovUOa",
    "lastExecutedAt": 1699991736355,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "print(data.review[10])",
    "outputId": "08febb58-39f7-4a34-a49d-088c0425bf8d",
    "outputsMetadata": {
     "0": {
      "height": 329,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I loved this movie from beginning to end.I am a musician and i let drugs get in the way of my some of the things i used to love(skateboarding,drawing) but my friends were always there for me.Music was like my rehab,life support,and my drug.It changed my life.I can totally relate to this movie and i wish there was more i could say.This movie left me speechless to be honest.I just saw it on the Ifc channel.I usually hate having satellite but this was a perk of having satellite.The ifc channel shows some really great movies and without it I never would have found this movie.Im not a big fan of the international films because i find that a lot of the don't do a very good job on translating lines.I mean the obvious language barrier leaves you to just believe thats what they are saying but its not that big of a deal i guess.I almost never got to see this AMAZING movie.Good thing i stayed up for it instead of going to bed..well earlier than usual.lol.I hope you all enjoy the hell of this movie and Love this movie just as much as i did.I wish i could type this all in caps but its again the rules i guess thats shouting but it would really show my excitement for the film.I Give It Three Thumbs Way Up!<br /><br />This Movie Blew ME AWAY!\n"
     ]
    }
   ],
   "source": [
    "print(data.review[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91878eae-5a77-4385-a906-9a24d123ca32",
   "metadata": {
    "id": "lAvczEBgxUWl"
   },
   "source": [
    "**Question 3:** Let's define a function that would clean each movie review (sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11d17e2d-dd11-463e-be56-3a1b3c4d338f",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "id": "eKKIsHqZwRJR",
    "lastExecutedAt": 1699991736403,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\n\nenglish_stopwords = stopwords.words('english')\nstemmer = PorterStemmer()\n\n# define cleaning function\ndef clean_review(text):\n    text = text.lower()\n    text = re.sub(r'[^a-z]', ' ', text)\n    tokens = word_tokenize(text)\n    stemmed = [stemmer.stem(word) for word in tokens]\n    text = ' '.join(stemmed)\n    text = ' '.join([word for word in text.split() if word not in english_stopwords])\n    return text"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# define cleaning function\n",
    "def clean_review(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z]', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    text = ' '.join(stemmed)\n",
    "    text = ' '.join([word for word in text.split() if word not in english_stopwords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dcbda5-ea8a-4a72-a7f0-3afb1a074ba9",
   "metadata": {
    "id": "-NIqPBfK67Zc"
   },
   "source": [
    "**Question 4 :** Try it out on an instance of the dataset then on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d28f3f7c-99d5-40a3-9979-a7170b4dde09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionCancelledAt": null,
    "executionTime": 45,
    "id": "W4Bn3r1wzvwR",
    "lastExecutedAt": 1699991736448,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from colorama import Fore\n\ninstance = data.iloc[0]\nreview = instance['review']\ncleaned_review = clean_review(review)\n\nprint(Fore.RED + 'Original review:', end=' ')\nprint(Fore.RESET + review)\nprint(Fore.GREEN + 'Cleaned review:', end=' ')\nprint(Fore.RESET + cleaned_review)",
    "outputId": "9512668c-908d-45bf-f325-a8339181ad3d",
    "outputsMetadata": {
     "0": {
      "height": 368,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mOriginal review: \u001b[39mI went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great, and I suggest that you go see it before you judge.\n",
      "\u001b[32mCleaned review: \u001b[39mwent saw thi movi last night coax friend mine admit wa reluct see becaus knew ashton kutcher wa onli abl comedi wa wrong kutcher play charact jake fischer veri well kevin costner play ben randal profession sign good movi toy emot thi one exactli entir theater wa sold wa overcom laughter dure first half movi move tear dure second half exit theater onli saw mani women tear mani full grown men well tri desper let anyon see cri thi movi wa great suggest go see befor judg\n"
     ]
    }
   ],
   "source": [
    "from colorama import Fore\n",
    "\n",
    "instance = data.iloc[0]\n",
    "review = instance['review']\n",
    "cleaned_review = clean_review(review)\n",
    "\n",
    "print(Fore.RED + 'Original review:', end=' ')\n",
    "print(Fore.RESET + review)\n",
    "print(Fore.GREEN + 'Cleaned review:', end=' ')\n",
    "print(Fore.RESET + cleaned_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c6079b-5fb2-4e6c-8200-bc795cbea29e",
   "metadata": {
    "id": "24Ycze9C6_yb"
   },
   "source": [
    "And now clean the entire dataset reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "774f21bf-aaa5-45f7-885f-385ccdddc002",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionCancelledAt": null,
    "executionTime": 233459,
    "id": "6kHxWkPTz5eA",
    "lastExecutedAt": 1699991969907,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# apply to all dataset\ndata['clean_review'] = data['review'].apply(clean_review)\ndata.head()",
    "outputId": "095ecae2-e124-4d51-b85c-bfb331f2c565"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "      <td>went saw thi movi last night coax friend mine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "      <td>actor turn director bill paxton follow hi prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "      <td>recreat golfer knowledg sport histori wa pleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>1</td>\n",
       "      <td>saw thi film sneak preview delight cinematogra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "      <td>bill paxton ha taken true stori us golf open m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  I went and saw this movie last night after bei...          1   \n",
       "1  Actor turned director Bill Paxton follows up h...          1   \n",
       "2  As a recreational golfer with some knowledge o...          1   \n",
       "3  I saw this film in a sneak preview, and it is ...          1   \n",
       "4  Bill Paxton has taken the true story of the 19...          1   \n",
       "\n",
       "                                        clean_review  \n",
       "0  went saw thi movi last night coax friend mine ...  \n",
       "1  actor turn director bill paxton follow hi prom...  \n",
       "2  recreat golfer knowledg sport histori wa pleas...  \n",
       "3  saw thi film sneak preview delight cinematogra...  \n",
       "4  bill paxton ha taken true stori us golf open m...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply to all dataset\n",
    "data['clean_review'] = data['review'].apply(clean_review)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8138934-5236-4456-beba-742ad2d5f39a",
   "metadata": {
    "id": "zkVqSSzu2Ax8"
   },
   "source": [
    "## Split dataset for training and testing\n",
    "We will split our data into two subsets: a 50% subset will be used for training the model for prediction and the remaining 50% will be used for evaluating or testing its performance. The random state ensures reproducibility of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1925c2-64d3-4136-a14d-6e4c95b7694c",
   "metadata": {
    "id": "HfMQ4DP0LahH"
   },
   "source": [
    "**Question 5:** Split your data to get x_train, x_test, y_train and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e5ba1a9-c83c-44c3-be54-9ceddd2ab572",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 52,
    "id": "QPHlwVS71brN",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1699991969960,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from sklearn.model_selection import train_test_split\n\nX = data['clean_review']\ny = data['sentiment']\n\n# Split data into 50% training & 50% test\n# Use a random state of 42 for example to ensure having the same split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)",
    "outputId": "6a3d07a3-d953-44c2-965d-eea7b79582d5",
    "outputsMetadata": {
     "0": {
      "height": 76,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n",
      "(25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['clean_review']\n",
    "y = data['sentiment']\n",
    "\n",
    "# Split data into 50% training & 50% test\n",
    "# Use a random state of 42 for example to ensure having the same split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9aebd-4414-440e-81ba-f97df5d7aeeb",
   "metadata": {
    "id": "Wz23g0nD2nhN"
   },
   "source": [
    "## Feature extraction with Bag of Words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02872fb1-85b6-465a-a8c7-5de58c31ef12",
   "metadata": {
    "id": "FGHs66FILldh"
   },
   "source": [
    "**Question 6:**  In this section, apply the Bag of Words method to learn the vocabulary of your text and with it transform your training input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be3ba391-194a-4214-b584-a41b715be880",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionCancelledAt": null,
    "executionTime": 5133,
    "id": "0_B0vrn-2sON",
    "lastExecutedAt": 1699991975093,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from sklearn.feature_extraction.text import CountVectorizer\n\n# define a CountVectorizer (with binary=True and max_features=10000)\nvectorizer = CountVectorizer(binary=True, max_features=10000)\n\n# learn the vocabulary of all tokens in our training dataset\nvectorizer.fit(X_train)\n\n# transform x_train to bag of words\nX_train_bow = vectorizer.transform(X_train)\nX_test_bow = vectorizer.transform(X_test)\n\nprint(X_train_bow.shape, y_train.shape)\nprint(X_test_bow.shape, y_test.shape)",
    "outputId": "ab41d352-7a96-4a92-b999-5c19de129259",
    "outputsMetadata": {
     "0": {
      "height": 76,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 10000) (25000,)\n",
      "(25000, 10000) (25000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# define a CountVectorizer (with binary=True and max_features=10000)\n",
    "vectorizer = CountVectorizer(binary=True, max_features=10000)\n",
    "\n",
    "# learn the vocabulary of all tokens in our training dataset\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "# transform x_train to bag of words\n",
    "X_train_bow = vectorizer.transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "print(X_train_bow.shape, y_train.shape)\n",
    "print(X_test_bow.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8909aa-7509-4393-a6d4-6c1c12e041d7",
   "metadata": {
    "id": "UtLaJfuw4060"
   },
   "source": [
    "## Classification\n",
    "\n",
    "**Question 7:** Your data is ready for classification. For this task use [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab9110de-d1df-4dc0-b3d3-aa287ecba7bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionCancelledAt": null,
    "executionTime": 2341,
    "id": "9mS51YGO4hfv",
    "lastExecutedAt": 1699991977434,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from sklearn.linear_model import LogisticRegression\n\n# define the LogisticRegression classifier\nmodel = LogisticRegression(C=0.1)\n\n# train the classifier on the training data\nmodel.fit(X_train_bow, y_train)\n\n# get the mean accuracy on the training data\nacc_train = model.score(X_train_bow, y_train)\n\nprint('Training Accuracy:', acc_train)",
    "outputId": "752487b4-6cb9-46cb-8e10-9c6cbb60f125",
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     },
     "1": {
      "height": 329,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.94052\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# define the LogisticRegression classifier\n",
    "model = LogisticRegression(C=0.1)\n",
    "\n",
    "# train the classifier on the training data\n",
    "model.fit(X_train_bow, y_train)\n",
    "\n",
    "# get the mean accuracy on the training data\n",
    "acc_train = model.score(X_train_bow, y_train)\n",
    "\n",
    "print('Training Accuracy:', acc_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c34f20a-977f-44bc-8e99-12d51d5f6474",
   "metadata": {
    "id": "2Csw7GEm76E5"
   },
   "source": [
    "**Question 8:**  Evaluating the performance of your model through its accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a9e400c-4590-42ff-92e8-e468538af31b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionCancelledAt": null,
    "executionTime": 1811,
    "id": "sBJnyoqO5NyE",
    "lastExecutedAt": 1699991979245,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Evaluate model with test data\n# Transform the test data to bag of words\nX_test_bow = vectorizer.transform(X_test)\n\n# Get the mean accuracy on the test data\nacc_test = model.score(X_test_bow, y_test)\n\nprint('Test Accuracy:', acc_test)\n",
    "outputId": "924e32bf-28a6-4a38-d8cd-d4cff075ed63",
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.87884\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model with test data\n",
    "# Transform the test data to bag of words\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Get the mean accuracy on the test data\n",
    "acc_test = model.score(X_test_bow, y_test)\n",
    "\n",
    "print('Test Accuracy:', acc_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b45415-757a-4448-82ab-027931e7b9d0",
   "metadata": {
    "id": "Yh5927-d6Gq4"
   },
   "source": [
    "## Bonus: Let's use the model to predict!\n",
    "To do so, let's create a predict function which takes as argument your model and the bag of words vectorizer together with a review on which it would predict the sentiment. This review should be cleaned with the `clean_review` function we built, transformed by bag of words and then used for prediction with `model.predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef10b838-ee5b-4f02-9454-09022ecb43aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionCancelledAt": null,
    "executionTime": 47,
    "id": "u6kxkZ5m55Ii",
    "lastExecutedAt": 1699991979292,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# define predict function\ndef predict(model, vectorizer, cleaned_review):\n    # Transform the cleaned review to bag of words\n    review_bow = vectorizer.transform([cleaned_review])\n\n    # Predict the sentiment\n    prediction = model.predict(review_bow)\n\n    # Return the predicted sentiment (0 for negative, 1 for positive)\n    return prediction[0]\n\n# Example usage:\nreview = 'The movie was great!'\npredicted_sentiment = predict(model, vectorizer, clean_review(review))\n\nprint('Predicted Sentiment:', predicted_sentiment)\n",
    "outputId": "c856fd8e-75d2-4d72-a3d1-badfe920d1cf",
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: 1\n"
     ]
    }
   ],
   "source": [
    "# define predict function\n",
    "def predict(model, vectorizer, cleaned_review):\n",
    "    # Transform the cleaned review to bag of words\n",
    "    review_bow = vectorizer.transform([cleaned_review])\n",
    "\n",
    "    # Predict the sentiment\n",
    "    prediction = model.predict(review_bow)\n",
    "\n",
    "    # Return the predicted sentiment (0 for negative, 1 for positive)\n",
    "    return prediction[0]\n",
    "\n",
    "# Example usage:\n",
    "review = 'The movie was great!'\n",
    "predicted_sentiment = predict(model, vectorizer, clean_review(review))\n",
    "\n",
    "print('Predicted Sentiment:', predicted_sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f998a847-0a13-4244-aab2-64791ff69e25",
   "metadata": {
    "id": "7VrNunL18l4a"
   },
   "source": [
    "And let's try it out on an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61d72646-8d74-47dd-aea7-b6f50960a611",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionCancelledAt": null,
    "executionTime": 47,
    "id": "8z6WCl916flD",
    "lastExecutedAt": 1699991979339,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "review = 'The movie was great!'\npredict (model, vectorizer, review)",
    "outputId": "be344a41-515a-4c07-e582-e19202875a22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = 'The movie was great!'\n",
    "predict (model, vectorizer, review)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "editor": "DataCamp Workspace",
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
